{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My original video file without bounding boxes is org_video.mp4\n",
    "## My new video file with bounded boxes is new_video.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Using available pre-trained models for object detection, conduct inference on a short video (5-10 seconds) of a street scene drawing bounding boxes around detected vehicles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xk4FU-jx9kc3"
   },
   "outputs": [],
   "source": [
    "!pip install -U tensorflow>=2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "yn5_uV1HLvaz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IogyryF2lFBL"
   },
   "source": [
    "## Utility functions to convert video into frames and from frames back to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(video_path, output_folder):\n",
    "    if os.path.exists(output_folder):\n",
    "        return\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    success, image = video.read()\n",
    "    count = 0\n",
    "\n",
    "    while success:\n",
    "        cv2.imwrite(f\"{output_folder}/frame_{count}.jpg\", image)\n",
    "        success, image = video.read()\n",
    "        count += 1\n",
    "\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_video(frames_path, output_video_path):\n",
    "    if not os.path.exists(frames_path):\n",
    "        return\n",
    "\n",
    "    frame_paths = sorted([f\"{frames_path}/{frame}\" for frame in os.listdir(frames_path) if frame.endswith('.jpg')], key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    \n",
    "    first_frame = cv2.imread(frame_paths[0])\n",
    "    frame_height, frame_width, _ = first_frame.shape\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (frame_width, frame_height))\n",
    "\n",
    "    for frame_path in frame_paths:\n",
    "        frame = cv2.imread(frame_path)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "-y9R0Xllefec"
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "  Puts image into numpy array to feed into tensorflow graph.\n",
    "  Note that by convention we put it into a numpy array with shape\n",
    "  (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "  Args:\n",
    "    path: the file path to the image\n",
    "\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  image = None\n",
    "  if(path.startswith('http')):\n",
    "    response = urlopen(path)\n",
    "    image_data = response.read()\n",
    "    image_data = BytesIO(image_data)\n",
    "    image = Image.open(image_data)\n",
    "  else:\n",
    "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def save_image_from_numpy_array(image_array, output_path):\n",
    "    output_folder = os.path.dirname(output_path)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    if image_array.shape[-1] == 3:\n",
    "        image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    success = cv2.imwrite(output_path, image_array, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
    "\n",
    "ALL_MODELS = {\n",
    "'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
    "'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
    "'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
    "'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
    "'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
    "'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
    "'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
    "'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
    "'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
    "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
    "'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
    "'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
    "'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
    "'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
    "'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
    "'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
    "'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
    "'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
    "'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
    "'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
    "'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
    "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
    "'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
    "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
    "'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
    "'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
    "'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
    "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
    "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
    "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
    "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
    "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
    "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
    "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
    "'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
    "'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
    "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
    "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
    "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
    "}\n",
    "\n",
    "IMAGES_FOR_TEST = {\n",
    "  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n",
    "  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n",
    "  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
    "  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n",
    "  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
    "  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n",
    "  # By Am√©rico Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
    "  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n",
    "  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
    "  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n",
    "  # Source: https://upload.wikimedia.org/wikipedia/commons/7/72/GB_London_Street_View_12.jpg\n",
    "  'Street' : 'https://upload.wikimedia.org/wikipedia/commons/7/72/GB_London_Street_View_12.jpg',\n",
    "}\n",
    "\n",
    "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n",
    " (0, 2),\n",
    " (1, 3),\n",
    " (2, 4),\n",
    " (0, 5),\n",
    " (0, 6),\n",
    " (5, 7),\n",
    " (7, 9),\n",
    " (6, 8),\n",
    " (8, 10),\n",
    " (5, 6),\n",
    " (5, 11),\n",
    " (6, 12),\n",
    " (11, 12),\n",
    " (11, 13),\n",
    " (13, 15),\n",
    " (12, 14),\n",
    " (14, 16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14bNk1gzh0TN"
   },
   "source": [
    "## Visualization tools\n",
    "\n",
    "To visualize the images with the proper detected boxes, keypoints and segmentation, we will use the TensorFlow Object Detection API. To install it we will clone the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi28cqGGFWnY"
   },
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository\n",
    "!git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yX3pb_pXDjYA"
   },
   "source": [
    "Intalling the Object Detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwdsBdGhFanc"
   },
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#sudo apt install -y protobuf-compiler\n",
    "#cd models/research/\n",
    "#protoc object_detection/protos/*.proto --python_out=.\n",
    "#cp object_detection/packages/tf2/setup.py .\n",
    "#python -m pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yDNgIx-kV7X"
   },
   "source": [
    "Now we can import the dependencies we will need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "2JCeQU3fkayh"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKtD0IeclbL5"
   },
   "source": [
    "### Load label map data (for plotting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "5mucYUS6exUJ"
   },
   "outputs": [],
   "source": [
    "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6917xnUSlp9x"
   },
   "source": [
    "## Build a detection model and load pre-trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "HtwrSqvakTNn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model:Mask R-CNN Inception ResNet V2 1024x1024\n",
      "Model Handle at TensorFlow Hub: https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1\n"
     ]
    }
   ],
   "source": [
    "model_display_name = 'Mask R-CNN Inception ResNet V2 1024x1024'\n",
    "model_handle = ALL_MODELS[model_display_name]\n",
    "\n",
    "print('Selected model:'+ model_display_name)\n",
    "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muhUt-wWL582"
   },
   "source": [
    "## Loading the selected model from TensorFlow Hub\n",
    "\n",
    "Here we just need the model handle that was selected and use the Tensorflow Hub library to load it to memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "rBuD07fLlcEO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "print('loading model...')\n",
    "hub_model = hub.load(model_handle)\n",
    "print('model loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIawRDKPPnd4"
   },
   "source": [
    "## Loading an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "hX-AWUQ1wIEr"
   },
   "outputs": [],
   "source": [
    "selected_image = \"Street\"\n",
    "flip_image_horizontally = False\n",
    "convert_image_to_grayscale = False\n",
    "\n",
    "def load_image(image_path, show_img=False):\n",
    "    image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "    if(flip_image_horizontally):\n",
    "        image_np[0] = np.fliplr(image_np[0]).copy()\n",
    "\n",
    "    if(convert_image_to_grayscale):\n",
    "        image_np[0] = np.tile(np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "    if show_img:\n",
    "        plt.figure(figsize=(24,32))\n",
    "        plt.imshow(image_np[0])\n",
    "        plt.show()\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ5VYaBoeeFM"
   },
   "source": [
    "## Doing the inference and Visualizing the results\n",
    "\n",
    "To do the inference we just need to call our TF Hub loaded model.\n",
    "\n",
    "Here is where we will need the TensorFlow Object Detection API to show the squares from the inference step (and the keypoints when available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "2O7rV8g9s8Bz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_inference_and_visualize_result(image_np, show_img=False):\n",
    "    \n",
    "    results = hub_model(image_np)\n",
    "\n",
    "    result = {key:value.numpy() for key,value in results.items()}\n",
    "    #print(result.keys())\n",
    "\n",
    "    label_id_offset = 0\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    keypoints, keypoint_scores = None, None\n",
    "    if 'detection_keypoints' in result:\n",
    "      keypoints = result['detection_keypoints'][0]\n",
    "      keypoint_scores = result['detection_keypoint_scores'][0]\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections[0],\n",
    "          result['detection_boxes'][0],\n",
    "          (result['detection_classes'][0] + label_id_offset).astype(int),\n",
    "          result['detection_scores'][0],\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=200,\n",
    "          min_score_thresh=.30,\n",
    "          agnostic_mode=False,\n",
    "          keypoints=keypoints,\n",
    "          keypoint_scores=keypoint_scores,\n",
    "          keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n",
    "    \n",
    "    if show_img:\n",
    "        plt.figure(figsize=(24,32))\n",
    "        plt.imshow(image_np_with_detections[0])\n",
    "        plt.show()\n",
    "    return image_np_with_detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'org_video.mp4'\n",
    "output_folder = 'frames'\n",
    "video_to_frames(video_path, output_folder)\n",
    "   \n",
    "files = os.listdir(output_folder)\n",
    "file_paths = [os.path.join(output_folder, file) for file in files]\n",
    "sorted_file_paths = sorted(file_paths, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "for image_path in sorted_file_paths:\n",
    "    image_np = load_image(image_path)\n",
    "    new_path = image_path.replace(\"frames\", \"new_frames\")\n",
    "    image_np = do_inference_and_visualize_result(image_np)\n",
    "    save_image_from_numpy_array(image_np, new_path)\n",
    "\n",
    "frames_to_video(\"new_frames\", \"new_video.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My original video file without bounding boxes is org_video.mp4\n",
    "## My new video file with bounded boxes is new_video.mp4"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "demo05_Object_Detection_TF2_Hub",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
